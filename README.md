# ğŸŒ‰ **PuenteLLM-MCP**

<div align="center">

[![Python Version](https://img.shields.io/badge/python-3.10+-blue.svg)](https://python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-orange.svg)](https://modelcontextprotocol.io/)
[![GUI Framework](https://img.shields.io/badge/GUI-CustomTkinter-purple.svg)](https://github.com/TomSchimansky/CustomTkinter)

**Una aplicaciÃ³n de escritorio moderna que conecta modelos de lenguaje con capacidades extendidas a travÃ©s del Protocolo de Contexto de Modelos (MCP)**

[ğŸš€ CaracterÃ­sticas](#-caracterÃ­sticas-principales) â€¢ [ğŸ“¦ InstalaciÃ³n](#-instalaciÃ³n) â€¢ [ğŸ› ï¸ Uso](#ï¸-uso) â€¢ [ğŸ§ª Testing](#-testing) â€¢ [ğŸ“‹ ConfiguraciÃ³n](#-configuraciÃ³n)

</div>

---

## ğŸ“– **Â¿QuÃ© es PuenteLLM-MCP?**

**PuenteLLM-MCP** es una aplicaciÃ³n de escritorio desarrollada en Python que actÃºa como puente inteligente entre:

- **ğŸ¤– Modelos de Lenguaje (LLM)** - Locales (Ollama) y remotos (OpenRouter, OpenAI, etc.)
- **ğŸ”§ Servidores MCP** - Para acceso seguro a archivos, datos y herramientas externas
- **ğŸ‘¤ Usuario** - A travÃ©s de una interfaz grÃ¡fica moderna y intuitiva

### **Â¿QuÃ© es MCP?**
El **Protocolo de Contexto de Modelos (MCP)** es un estÃ¡ndar abierto que permite a las aplicaciones proporcionar contexto y capacidades a los modelos de lenguaje de manera unificada. Es como un "puerto USB-C" para IA que estandariza la comunicaciÃ³n entre LLMs y herramientas externas.

---

## â­ **CaracterÃ­sticas principales**

### ğŸ¯ **Core Features**
- **ğŸ’¬ Chat conversacional** con mÃºltiples proveedores de LLM
- **ğŸ”— IntegraciÃ³n MCP** con el SDK oficial para mÃ¡xima compatibilidad
- **ğŸ›ï¸ GestiÃ³n de servidores MCP** desde la interfaz (iniciar/detener/configurar)
- **ğŸ”„ Cambio de modelo en caliente** con persistencia de configuraciÃ³n
- **â¹ï¸ Control de respuestas** con botÃ³n de parada durante la generaciÃ³n

### ğŸ›¡ï¸ **Seguridad y ConfiguraciÃ³n**
- **ğŸ” GestiÃ³n segura de credenciales** con variables de entorno
- **âš™ï¸ ConfiguraciÃ³n persistente** de preferencias y modelos
- **ğŸŒ Soporte multi-proveedor** (Ollama, OpenRouter, OpenAI, Anthropic)
- **ğŸ” BÃºsqueda inteligente de modelos** con filtrado en tiempo real

### ğŸ¨ **Experiencia de Usuario**
- **ğŸ–¥ï¸ Interfaz moderna** con CustomTkinter
- **ğŸ“± DiseÃ±o responsive** y tema adaptable
- **ğŸš€ Arranque automÃ¡tico** de servicios (Ollama)
- **ğŸ¯ Respuestas optimizadas** con configuraciÃ³n automÃ¡tica

---

## ğŸ› ï¸ **InstalaciÃ³n**

### **Prerrequisitos**
- **Python 3.10+** ([Descargar](https://python.org/downloads/))
- **[Ollama](https://ollama.com/)** (para modelos locales)
- **Node.js** (para algunos servidores MCP)

### **1. Clonar el repositorio**
```bash
git clone https://github.com/mnperrone/puentellm-mcp.git
cd puentellm-mcp
```

### **2. Crear entorno virtual**
```bash
python -m venv .venv
# En Windows:
.venv\Scripts\activate
# En macOS/Linux:
source .venv/bin/activate
```

### **3. Instalar dependencias**
```bash
pip install -r requirements.txt
```

### **4. Configurar credenciales (opcional)**
```bash
cp .env.example .env
# Editar .env con tus API keys para proveedores remotos
```

---

## ğŸš€ **Uso**

### **Iniciar la aplicaciÃ³n**
```bash
python desktop_app.py
```

### **Primera configuraciÃ³n**

1. **ğŸ”§ Configurar proveedor LLM:**
   - Ir a **"ConfiguraciÃ³n" â†’ "ConfiguraciÃ³n LLM Remoto"**
   - Seleccionar proveedor (Ollama, OpenRouter, etc.)
   - Ingresar credenciales si es necesario
   - **Probar conexiÃ³n** y seleccionar modelo

2. **âš™ï¸ Configurar servidores MCP:**
   - Ir a **"ConfiguraciÃ³n" â†’ "ConfiguraciÃ³n MCP"** 
   - Agregar/editar servidores MCP
   - Iniciar servidores necesarios

3. **ğŸ’¬ Â¡Comenzar a chatear!**
   - Escribir en el campo de chat
   - Usar comandos MCP cuando estÃ©n disponibles
   - El LLM puede acceder a capacidades de los servidores MCP automÃ¡ticamente

### **BÃºsqueda de modelos**
Con mÃ¡s de 340+ modelos disponibles en algunos proveedores:
- **ğŸ” Campo de bÃºsqueda** inteligente en configuraciÃ³n
- **Filtrado en tiempo real** mientras escribes
- **Ejemplos:** `gpt-4`, `claude`, `free`, `deepseek`

---

## ğŸ§ª **Testing**

### **Ejecutar tests**
```bash
cd tests
pip install -r requirements.txt
python run_tests.py
```

### **Tests disponibles**
- **âœ… ValidaciÃ³n de configuraciÃ³n** - Estructura y consistencia de config files
- **ğŸ”Œ Conexiones MCP** - VerificaciÃ³n de conectividad con servidores
- **âš™ï¸ GestiÃ³n de configuraciÃ³n** - Persistencia y carga de configuraciones
- **ğŸ§ª Handlers LLM** - InicializaciÃ³n y comunicaciÃ³n con proveedores

### **Tests de integraciÃ³n**
```bash
# Test especÃ­fico de configuraciÃ³n
python tests/test_config_validation.py

# Test de conexiÃ³n MCP
python tests/test_mcp_connection.py
```

---

## ğŸ“‹ **ConfiguraciÃ³n**

### **Estructura de archivos**
```
puentellm-mcp/
â”œâ”€â”€ ğŸ“ llm_providers/          # Handlers para diferentes LLM providers
â”œâ”€â”€ ğŸ“ tests/                  # Suite de testing
â”œâ”€â”€ ğŸ“ logs/                   # Logs de aplicaciÃ³n
â”œâ”€â”€ ğŸ“„ app_config.json         # ConfiguraciÃ³n principal
â”œâ”€â”€ ğŸ“„ .env                    # Credenciales (git-ignored)
â”œâ”€â”€ ğŸ“„ mcp_servers.json        # ConfiguraciÃ³n de servidores MCP
â””â”€â”€ ğŸ“„ desktop_app.py          # Punto de entrada principal
```

### **Variables de entorno**
Crear `.env` basado en `.env.example`:
```bash
# OpenRouter
OPENROUTER_API_KEY=your_api_key_here

# OpenAI
OPENAI_API_KEY=your_api_key_here

# Anthropic
ANTHROPIC_API_KEY=your_api_key_here
```

### **ConfiguraciÃ³n MCP**
El archivo `mcp_servers.json` define los servidores MCP disponibles:
```json
{
  "mcpServers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/ruta/permitida"]
    }
  }
}
```

---

## ğŸ—ï¸ **Arquitectura**

### **Componentes principales**

| Componente | DescripciÃ³n | Archivo |
|------------|-------------|---------|
| **ğŸ–¥ï¸ UI Principal** | Interfaz de usuario y orquestaciÃ³n | `desktop_app.py`, `chat_app.py` |
| **ğŸ¤– LLM Bridge** | AbstracciÃ³n para mÃºltiples proveedores | `llm_bridge.py` |
| **ğŸ”— MCP Manager** | GestiÃ³n de servidores MCP | `mcp_manager.py` |
| **âš™ï¸ Config Manager** | Persistencia de configuraciÃ³n | `app_config.py` |
| **ğŸ” Env Manager** | GestiÃ³n segura de credenciales | `env_manager.py` |

### **Flujo de datos**
```
Usuario â†’ UI â†’ LLM Bridge â†’ Proveedor LLM
                â†“
         MCP Manager â†’ Servidor MCP â†’ Herramientas
```

---

## ğŸ¤ **Contribuir**

1. **Fork** el repositorio
2. **Crear** una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. **Commit** tus cambios (`git commit -am 'Agregar nueva funcionalidad'`)
4. **Push** a la rama (`git push origin feature/nueva-funcionalidad`)
5. **Crear** un Pull Request

---

## ğŸ“„ **Licencia**

Este proyecto estÃ¡ licenciado bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.

---

## ğŸ†˜ **Soporte**

- **ğŸ“‹ Issues:** [GitHub Issues](https://github.com/mnperrone/puentellm-mcp/issues)
- **ğŸ’¬ Discusiones:** [GitHub Discussions](https://github.com/mnperrone/puentellm-mcp/discussions)
- **ğŸ“§ Email:** Contacto a travÃ©s de GitHub

---

<div align="center">

**â­ Si este proyecto te resulta Ãºtil, Â¡no olvides darle una estrella!**

[ğŸ” Volver arriba](#-puentellm-mcp)

</div>

O instala todo de una vez con:
```bash
pip install customtkinter==5.2.2 ollama psutil mcp httpx "pydantic>=2.11.0,<3.0.0" pydantic-settings>=2.5.2 python-multipart>=0.0.9 sse-starlette>=1.6.1 starlette>=0.27 uvicorn>=0.31.1 strictjson darkdetect pywin32>=310
```

## Estructura del proyecto
```
chat_app.py        # LÃ³gica principal de la app y orquestaciÃ³n de mÃ³dulos
ui_helpers.py      # Utilidades de UI y logging en el chat
dialogs.py         # DiÃ¡logos para herramientas y argumentos
llm_bridge.py      # AbstracciÃ³n y manejo de LLM/Ollama
llm_mcp_handler.py # Manejo de comandos MCP generados por el LLM
mcp_sdk_bridge.py  # IntegraciÃ³n con el SDK oficial de MCP
mcp_manager.py     # GestiÃ³n de procesos de servidores MCP
app_config.py      # Persistencia de configuraciÃ³n y preferencias
last_llm_model.txt # Archivo de persistencia del Ãºltimo modelo LLM usado
mcp_servers.json   # ConfiguraciÃ³n de servidores MCP
LICENSE            # Licencia MIT
README.md          # Este archivo
```

## Uso
1. **Inicia Ollama** en tu mÃ¡quina (o usa el menÃº LLM > Iniciar servicio Ollama).
2. Ejecuta la app:
   ```bash
   python desktop_app.py
   ```
3. Escribe tu mensaje en el campo inferior y presiona Enter o el botÃ³n "Enviar".
4. Usa el menÃº MCP para cargar o gestionar servidores MCP, descubrir y ejecutar herramientas vÃ­a SDK.
5. Cambia el modelo LLM desde el menÃº LLM si lo deseas.
6. Si la respuesta es muy larga, puedes interrumpirla con el botÃ³n "Detener respuesta".

## PersonalizaciÃ³n
- Edita `mcp_servers.json` para agregar o modificar servidores MCP.
- El comportamiento del asistente se puede ajustar en el mÃ©todo `get_base_system_prompt` de `chat_app.py`.
- Puedes ampliar la persistencia de configuraciÃ³n en `app_config.py`.

## Notas
- El foco del cursor se posiciona automÃ¡ticamente en el campo de entrada al iniciar la app.
- El asistente responde solo en espaÃ±ol y de forma concisa.
- El proyecto no requiere carpetas `.venv` ni `.idea` para funcionar.

## Pruebas del sistema PuenteLLM-MCP

Este directorio contiene pruebas unitarias y scripts de prueba para el sistema PuenteLLM-MCP.

### Estructura del directorio de pruebas

```
tests/
â”œâ”€â”€ test_mcp_config_validation.py    # Pruebas para validaciÃ³n de configuraciÃ³n MCP
â”œâ”€â”€ test_mcp_connection.py          # Pruebas para conexiÃ³n con servidores MCP
â”œâ”€â”€ run_tests.py                    # Script para ejecutar todas las pruebas
â”œâ”€â”€ test_config.json                # Archivo de configuraciÃ³n de prueba
â”œâ”€â”€ test_script.py                  # Script de prueba para uso directo de las funciones
â””â”€â”€ requirements.txt                # Requisitos para las pruebas
```

### ConfiguraciÃ³n de pruebas

1. **Instalar dependencias**:
   ```bash
   pip install -r requirements.txt
   ```

2. **Configurar archivos de prueba**:
   - El archivo `test_config.json` define la configuraciÃ³n bÃ¡sica de servidores MCP para pruebas
   - AsegÃºrate de que los comandos y rutas en el archivo de configuraciÃ³n sean vÃ¡lidos para tu entorno

3. **Ejecutar pruebas**:
   ```bash
   python run_tests.py
   ```

### Tipos de pruebas

#### 1. ValidaciÃ³n de configuraciÃ³n (`test_mcp_config_validation.py`)

Estas pruebas verifican que la configuraciÃ³n de los servidores MCP sea correcta:
- Campos requeridos por tipo de servidor
- Valores vÃ¡lidos para tipos, puertos, comandos
- ValidaciÃ³n de configuraciones al aÃ±adir o actualizar servidores

#### 2. ConexiÃ³n con servidores (`test_mcp_connection.py`)

Estas pruebas verifican la capacidad de conexiÃ³n con distintos tipos de servidores MCP:
- Carga correcta de la configuraciÃ³n
- Inicio y detenciÃ³n de servidores locales
- Inicio y detenciÃ³n de servidores NPM
- ConexiÃ³n a servidores remotos
- ObtenciÃ³n y validaciÃ³n de lista de servidores

## Proveedores de LLM soportados

- **Ollama** (local, por defecto)
- **OpenAI Compatible** (API compatible, configurable)
- **Qwen** (Dashscope)

Todos los handlers de LLM implementan los mÃ©todos `generate(prompt)` y `stream(messages)` para compatibilidad total con el flujo de la app.

## Carpeta llm_providers

Contiene los mÃ³dulos para cada proveedor de LLM:
- `ollama_handler.py`: Handler para Ollama local
- `openai_compatible_handler.py`: Handler para APIs OpenAI compatibles
- `qwen_handler.py`: Handler para Qwen/Dashscope
- `llm_exception.py`: Excepciones personalizadas para errores de conexiÃ³n LLM
- `__init__.py`: Selector dinÃ¡mico de handler segÃºn proveedor

## IntegraciÃ³n con OpenRouter â€” sanitizaciÃ³n y manejo de rate-limits

Se ha aÃ±adido soporte mejorado para proveedores remotos tipo OpenRouter con dos mejoras importantes:

- SanitizaciÃ³n y "auto-space": algunos modelos (por ejemplo DeepSeek) devuelven tokens con marcadores subword o palabras concatenadas. El proyecto ahora incluye:
   - Un sanitizador conservador que reemplaza el marcador subword `â–`, elimina tokens de control entre `<...>` y colapsa espacios.
   - Una opciÃ³n opt-in llamada `auto_space_model_output` que intenta insertar espacios en casos donde el modelo devuelva palabras concatenadas. La heurÃ­stica es conservadora y utiliza una segmentaciÃ³n basada en un pequeÃ±o diccionario de alta frecuencia en espaÃ±ol para evitar particiones incorrectas.
   - La opciÃ³n puede activarse desde la UI en `ConfiguraciÃ³n de LLM Remoto` (casilla "Intentar corregir espacios faltantes en la salida del modelo (auto-space)") o por la variable de entorno `PUENTE_ENABLE_AUTO_SPACING=1`.

- Manejo de HTTP 429 (rate limits) en streaming:
   - El handler de OpenRouter ahora implementa reintentos explÃ­citos para respuestas 429, respeta el header `Retry-After` cuando estÃ© presente y aplica backoff exponencial con jitter. Esto reduce la probabilidad de fallos visibles para el usuario cuando el servicio responde temporalmente con rate limits.
   - Si tras varios reintentos el servidor sigue devolviendo 429, la app lanzarÃ¡ un error informativo: "OpenRouter rate limit (HTTP 429). Espera unos segundos o revisa tu cuota/API key."

Notas importantes:
- La autocorrecciÃ³n de espacios es conservadora; si observas divisiones errÃ³neas o no deseadas, desactÃ­vala desde la UI o poniendo `PUENTE_ENABLE_AUTO_SPACING=0`.
- Si recibes muchos 429 frecuentemente, revisa la cuota/plan de la API key de OpenRouter, reduce la tasa de peticiones desde la app, o utiliza otro proveedor.


## Buenas prÃ¡cticas y mantenimiento

- La interfaz de los handlers estÃ¡ unificada (`generate` y `stream`).
- El cÃ³digo estÃ¡ modularizado y documentado.
- Se recomienda mantener actualizados los requisitos en `requirements.txt` y revisar la documentaciÃ³n de cada proveedor MCP/LLM.

---

Para dudas, sugerencias o reportes, me puedes contactar en mnperrone@gmail.com
