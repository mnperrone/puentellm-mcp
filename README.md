# ğŸŒ‰ **PuenteLLM-MCP**

<div align="center">

[![Python Version](https://img.shields.io/badge/python-3.10+-blue.svg)](https://python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-orange.svg)](https://modelcontextprotocol.io/)
[![GUI Framework](https://img.shields.io/badge/GUI-CustomTkinter-purple.svg)](https://github.com/TomSchimansky/CustomTkinter)

**Una aplicaciÃ³n de escritorio moderna que conecta modelos de lenguaje con capacidades extendidas a travÃ©s del Protocolo de Contexto de Modelos (MCP)**

[ğŸš€ CaracterÃ­sticas](#-caracterÃ­sticas-principales) â€¢ [ğŸ“¦ InstalaciÃ³n](#-instalaciÃ³n) â€¢ [ğŸ› ï¸ Uso](#ï¸-uso) â€¢ [ğŸ§ª Testing](#-testing) â€¢ [ğŸ“‹ ConfiguraciÃ³n](#-configuraciÃ³n)

</div>

---

## ğŸ“– **Â¿QuÃ© es PuenteLLM-MCP?**

**PuenteLLM-MCP** es una aplicaciÃ³n de escritorio desarrollada en Python que actÃºa como puente inteligente entre:

- **ğŸ¤– Modelos de Lenguaje (LLM)** - Locales (Ollama) y remotos (OpenRouter, OpenAI, etc.)
- **ğŸ”§ Servidores MCP** - Para acceso seguro a archivos, datos y herramientas externas
- **ğŸ‘¤ Usuario** - A travÃ©s de una interfaz grÃ¡fica moderna y intuitiva

### **Â¿QuÃ© es MCP?**
El **Protocolo de Contexto de Modelos (MCP)** es un estÃ¡ndar abierto que permite a las aplicaciones proporcionar contexto y capacidades a los modelos de lenguaje de manera unificada. Es como un "puerto USB-C" para IA que estandariza la comunicaciÃ³n entre LLMs y herramientas externas.

---

## â­ **CaracterÃ­sticas principales**

### ğŸ¯ **Core Features**
- **ğŸ’¬ Chat conversacional** con mÃºltiples proveedores de LLM
- **ğŸ”— IntegraciÃ³n MCP** con el SDK oficial para mÃ¡xima compatibilidad
- **ğŸ›ï¸ GestiÃ³n de servidores MCP** desde la interfaz (iniciar/detener/configurar)
- **ğŸ”„ Cambio de modelo en caliente** con persistencia de configuraciÃ³n
- **â¹ï¸ Control de respuestas** con botÃ³n de parada durante la generaciÃ³n

### ğŸ›¡ï¸ **Seguridad y ConfiguraciÃ³n**
- **ğŸ” GestiÃ³n segura de credenciales** con variables de entorno
- **âš™ï¸ ConfiguraciÃ³n persistente** de preferencias y modelos
- **ğŸŒ Soporte multi-proveedor** (Ollama, OpenRouter, OpenAI, Anthropic)
- **ğŸ” BÃºsqueda inteligente de modelos** con filtrado en tiempo real

### ğŸ¨ **Experiencia de Usuario**
- **ğŸ–¥ï¸ Interfaz moderna** con CustomTkinter
- **ğŸ“± DiseÃ±o responsive** y tema adaptable
- **ğŸš€ Arranque automÃ¡tico** de servicios (Ollama)
- **ğŸ¯ Respuestas optimizadas** con configuraciÃ³n automÃ¡tica

---

## ğŸ› ï¸ **InstalaciÃ³n**

### **Prerrequisitos**
- **Python 3.10+** ([Descargar](https://python.org/downloads/))
- **[Ollama](https://ollama.com/)** (para modelos locales)
- **Node.js** (para algunos servidores MCP)

### **1. Clonar el repositorio**
```bash
git clone https://github.com/mnperrone/puentellm-mcp.git
cd puentellm-mcp
```

### **2. Instalar dependencias**
```bash
# Instalar paquetes principales (desde el directorio raÃ­z)
pip install customtkinter==5.2.2 ollama psutil mcp httpx "pydantic>=2.11.0,<3.0.0" pydantic-settings>=2.5.2 python-multipart>=0.0.9 sse-starlette>=1.6.1 starlette>=0.27 uvicorn>=0.31.1 strictjson darkdetect python-dotenv requests

# O en Windows con pywin32:
pip install customtkinter==5.2.2 ollama psutil mcp httpx "pydantic>=2.11.0,<3.0.0" pydantic-settings>=2.5.2 python-multipart>=0.0.9 sse-starlette>=1.6.1 starlette>=0.27 uvicorn>=0.31.1 strictjson darkdetect pywin32>=310 python-dotenv requests
```

### **3. Configurar credenciales (opcional)**
```bash
cp .env.example .env
# Editar .env con tus API keys para proveedores remotos
```

> **ğŸ’¡ Nota:** El proyecto **no requiere entorno virtual** para funcionar. Las dependencias se pueden instalar directamente en el sistema Python.

---

## ğŸš€ **Uso**

### **Iniciar la aplicaciÃ³n**
```bash
python desktop_app.py
```

### **Primera configuraciÃ³n**

1. **ğŸ”§ Configurar proveedor LLM:**
   - Ir a **"ConfiguraciÃ³n" â†’ "ConfiguraciÃ³n LLM Remoto"**
   - Seleccionar proveedor (Ollama, OpenRouter, etc.)
   - Ingresar credenciales si es necesario
   - **Probar conexiÃ³n** y seleccionar modelo

2. **âš™ï¸ Configurar servidores MCP:**
   - Ir a **"ConfiguraciÃ³n" â†’ "ConfiguraciÃ³n MCP"** 
   - Agregar/editar servidores MCP
   - Iniciar servidores necesarios

3. **ğŸ’¬ Â¡Comenzar a chatear!**
   - Escribir en el campo de chat
   - Usar comandos MCP cuando estÃ©n disponibles
   - El LLM puede acceder a capacidades de los servidores MCP automÃ¡ticamente

### **BÃºsqueda de modelos**
Con mÃ¡s de 340+ modelos disponibles en algunos proveedores:
- **ğŸ” Campo de bÃºsqueda** inteligente en configuraciÃ³n
- **Filtrado en tiempo real** mientras escribes
- **Ejemplos:** `gpt-4`, `claude`, `free`, `deepseek`

---

## ğŸ§ª **Testing**

### **Ejecutar tests**
```bash
cd tests
python run_tests.py
```

> **ğŸ’¡ Nota:** Los tests usan las mismas dependencias del proyecto principal, no requieren instalaciones adicionales.

### **Tests disponibles**
- **âœ… ValidaciÃ³n de configuraciÃ³n** - Estructura y consistencia de config files
- **ğŸ”Œ Conexiones MCP** - VerificaciÃ³n de conectividad con servidores
- **âš™ï¸ GestiÃ³n de configuraciÃ³n** - Persistencia y carga de configuraciones
- **ğŸ§ª Handlers LLM** - InicializaciÃ³n y comunicaciÃ³n con proveedores

### **Tests de integraciÃ³n**
```bash
# Test especÃ­fico de configuraciÃ³n
python tests/test_basic_structure.py

# Test de funcionalidad core
python tests/test_core_functionality.py
```

---

## ğŸ“‹ **ConfiguraciÃ³n**

### **Estructura de archivos**
```
puentellm-mcp/
â”œâ”€â”€ ğŸ“ llm_providers/          # Handlers para diferentes LLM providers
â”œâ”€â”€ ğŸ“ tests/                  # Suite de testing
â”œâ”€â”€ ğŸ“ logs/                   # Logs de aplicaciÃ³n
â”œâ”€â”€ ğŸ“„ app_config.json         # ConfiguraciÃ³n principal
â”œâ”€â”€ ğŸ“„ .env                    # Credenciales (git-ignored)
â”œâ”€â”€ ğŸ“„ mcp_servers.json        # ConfiguraciÃ³n de servidores MCP
â””â”€â”€ ğŸ“„ desktop_app.py          # Punto de entrada principal
```

### **Variables de entorno**
Crear `.env` basado en `.env.example`:
```bash
# OpenRouter
OPENROUTER_API_KEY=your_api_key_here

# OpenAI
OPENAI_API_KEY=your_api_key_here

# Anthropic
ANTHROPIC_API_KEY=your_api_key_here
```

### **ConfiguraciÃ³n MCP**
El archivo `mcp_servers.json` define los servidores MCP disponibles:
```json
{
  "mcpServers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/ruta/permitida"]
    }
  }
}
```

---

## ğŸ—ï¸ **Arquitectura**

### **Componentes principales**

| Componente | DescripciÃ³n | Archivo |
|------------|-------------|---------|
| **ğŸ–¥ï¸ UI Principal** | Interfaz de usuario y orquestaciÃ³n | `desktop_app.py`, `chat_app.py` |
| **ğŸ¤– LLM Bridge** | AbstracciÃ³n para mÃºltiples proveedores | `llm_bridge.py` |
| **ğŸ”— MCP Manager** | GestiÃ³n de servidores MCP | `mcp_manager.py` |
| **âš™ï¸ Config Manager** | Persistencia de configuraciÃ³n | `app_config.py` |
| **ğŸ” Env Manager** | GestiÃ³n segura de credenciales | `env_manager.py` |

### **Flujo de datos**
```
Usuario â†’ UI â†’ LLM Bridge â†’ Proveedor LLM
                â†“
         MCP Manager â†’ Servidor MCP â†’ Herramientas
```

---

## ğŸ¤ **Contribuir**

1. **Fork** el repositorio
2. **Crear** una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. **Commit** tus cambios (`git commit -am 'Agregar nueva funcionalidad'`)
4. **Push** a la rama (`git push origin feature/nueva-funcionalidad`)
5. **Crear** un Pull Request

---

## ğŸ“„ **Licencia**

Este proyecto estÃ¡ licenciado bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.

---

## ğŸ†˜ **Soporte**

- **ğŸ“‹ Issues:** [GitHub Issues](https://github.com/mnperrone/puentellm-mcp/issues)
- **ğŸ’¬ Discusiones:** [GitHub Discussions](https://github.com/mnperrone/puentellm-mcp/discussions)
- **ğŸ“§ Email:** Contacto a travÃ©s de GitHub

---

<div align="center">

**â­ Si este proyecto te resulta Ãºtil, Â¡no olvides darle una estrella!**

[ğŸ” Volver arriba](#-puentellm-mcp)

</div>
